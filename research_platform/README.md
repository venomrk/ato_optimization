# Multi-Agent Research Analysis Platform

A sophisticated, standalone platform for analyzing academic research papers using multiple advanced reasoning AI models. This system automatically discovers, downloads, processes, and analyzes research papers through collaborative multi-agent analysis with consensus building.

## ğŸŒŸ Key Features

### Paper Discovery & Extraction
- **5+ Paper Sources**: arXiv, PubMed, Semantic Scholar, CrossRef, Google Scholar
- **Auto-Download PDFs**: Automatically retrieves full-text PDFs when available
- **Advanced Extraction**: Extracts 10+ data fields including:
  - Title, authors, abstract, publication date
  - Experimental methodology and materials
  - Processing parameters (temperature, pressure, time)
  - Results and measurements
  - Equipment used
  - Citations and references

### Multi-Agent Reasoning (10+ Models)
Deploys independent AI agents using state-of-the-art reasoning models:
- **Claude 3** (Opus & Sonnet with extended thinking)
- **OpenAI** (GPT-4o, o1, o3 reasoning models)
- **Google Gemini 2.0** (with deep thinking)
- **DeepSeek R1** (reasoning model)
- **Qwen QwQ** (32B reasoning model)
- **Grok-2** (X.AI reasoning)
- **Yi-Lightning** (reasoning variant)
- **LLaMA 3 70B** and **Mixtral 8x22B**

Each agent independently:
- Analyzes papers from different perspectives
- Identifies patterns, contradictions, and insights
- Answers WHAT, HOW, WHY questions
- Generates confidence scores
- Cross-references findings

### Agent Collaboration & Consensus
- **Multi-Agent Debate**: Agents present findings and engage in structured dialogue
- **Weighted Voting**: Based on confidence scores and evidence quality
- **Contradiction Detection**: Identifies disagreements between agents
- **Consolidated Recommendations**: Synthesizes findings with confidence levels
- **Transparent Reasoning**: Shows individual agent thought processes

### Continuous Learning & Self-Updating
- **Model Monitoring**: Detects new model releases automatically
- **Auto-Registration**: Adds new reasoning models to agent pool
- **Knowledge Updates**: Periodically ingests new papers
- **Version Control**: Tracks all model versions used
- **Audit Trail**: Complete history of analyses

## ğŸ—ï¸ Architecture

```
research_platform/
â”œâ”€â”€ agents/              # Multi-agent system
â”‚   â”œâ”€â”€ base.py         # Agent interface
â”‚   â”œâ”€â”€ claude_agent.py # Claude 3 implementation
â”‚   â”œâ”€â”€ openai_agent.py # OpenAI implementation
â”‚   â”œâ”€â”€ gemini_agent.py # Google Gemini
â”‚   â”œâ”€â”€ generic_agent.py# Generic API agents
â”‚   â”œâ”€â”€ orchestrator.py # Agent coordination
â”‚   â””â”€â”€ consensus.py    # Consensus building
â”œâ”€â”€ extractors/         # Paper extraction
â”‚   â”œâ”€â”€ arxiv_extractor.py
â”‚   â”œâ”€â”€ pubmed_extractor.py
â”‚   â”œâ”€â”€ semantic_scholar_extractor.py
â”‚   â”œâ”€â”€ crossref_extractor.py
â”‚   â”œâ”€â”€ google_scholar_extractor.py
â”‚   â”œâ”€â”€ pdf_processor.py
â”‚   â””â”€â”€ orchestrator.py
â”œâ”€â”€ storage/            # Data persistence
â”‚   â”œâ”€â”€ database.py     # SQL storage
â”‚   â”œâ”€â”€ vector_store.py # Embeddings
â”‚   â””â”€â”€ cache.py        # Redis caching
â”œâ”€â”€ api/                # FastAPI server
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ config/             # Configuration
â”‚   â””â”€â”€ settings.py
â””â”€â”€ scripts/            # Utilities
```

**Stack:**
- **Backend**: FastAPI (Python 3.11+)
- **Agents**: LangChain, OpenAI, Anthropic, Google APIs
- **PDF Processing**: PyPDF2, pdfplumber, PyMuPDF
- **Storage**: PostgreSQL + ChromaDB (vector embeddings)
- **Caching**: Redis
- **Deployment**: Docker + Docker Compose

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.11+ (for local development)
- API keys for LLM providers

### Installation

1. **Clone and Navigate**
```bash
cd research_platform
```

2. **Configure Environment**
```bash
cp .env.example .env
# Edit .env with your API keys
```

Required API keys (add at least one provider):
```env
# At least one OpenAI key
OPENAI_API_KEY=sk-xxx

# At least one Anthropic key
ANTHROPIC_API_KEY=sk-ant-xxx

# At least one Google key
GOOGLE_API_KEY=AIzaxxx

# Optional additional providers
DEEPSEEK_API_KEY=sk-xxx
QWEN_API_KEY=xxx
XAI_API_KEY=xai-xxx
```

3. **Launch with Docker**
```bash
docker-compose up -d
```

The API will be available at `http://localhost:8000`

### Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm

# Run the server
python -m uvicorn api.app:create_app --factory --reload
```

## ğŸ“– Usage

### API Endpoints

#### 1. Search Papers
```bash
curl -X POST "http://localhost:8000/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "antimony fluorine doped tin oxide solar",
    "max_results": 20,
    "sources": ["arxiv", "semantic_scholar", "pubmed"],
    "download_pdfs": true
  }'
```

#### 2. Analyze Research Question
```bash
curl -X POST "http://localhost:8000/analyze" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "antimony fluorine doped tin oxide",
    "research_question": "What are the optimal doping concentrations for maximizing electrical conductivity while maintaining optical transparency?",
    "max_papers": 15,
    "run_full_analysis": true
  }'
```

#### 3. Check Agent Status
```bash
curl "http://localhost:8000/agents"
```

#### 4. View Recent Analyses
```bash
curl "http://localhost:8000/analyses?limit=10"
```

### Python Client Example

```python
import httpx
import asyncio

async def analyze_research():
    async with httpx.AsyncClient() as client:
        # Search for papers
        search_response = await client.post(
            "http://localhost:8000/search",
            json={
                "query": "quantum computing error correction",
                "max_results": 10,
                "download_pdfs": True
            }
        )
        papers = search_response.json()
        
        # Analyze with multi-agent system
        analysis_response = await client.post(
            "http://localhost:8000/analyze",
            json={
                "query": "quantum error correction",
                "research_question": "What are the most effective error correction codes for near-term quantum computers?",
                "max_papers": 10,
                "run_full_analysis": True
            },
            timeout=600.0
        )
        
        results = analysis_response.json()
        
        # Access consensus results
        consensus = results["analysis_results"]["general"]["consensus"]
        print(f"Confidence: {consensus['confidence_score']}")
        print(f"Key Findings: {consensus['key_findings']}")
        print(f"Recommendations: {consensus['recommendations']}")

asyncio.run(analyze_research())
```

## ğŸ¯ Analysis Types

The platform performs four types of analysis:

### WHAT Analysis
Focuses on:
- Key findings and results
- Materials and methods used
- Measurements reported
- Main conclusions

### HOW Analysis
Focuses on:
- Experimental procedures
- Material preparation methods
- Measurement techniques
- Methodology â†’ results pathway

### WHY Analysis
Focuses on:
- Method selection rationale
- Underlying mechanisms
- Significance of findings
- Limitations and contradictions

### GENERAL Analysis
Comprehensive analysis combining all aspects

## ğŸ“Š Output Format

```json
{
  "query": "research query",
  "papers_analyzed": 15,
  "agents_used": 12,
  "analysis_results": {
    "what": {
      "agent_responses": [
        {
          "agent_type": "claude-3-opus",
          "response": "...",
          "reasoning_chain": "...",
          "confidence_score": 0.85,
          "key_claims": [...],
          "evidence": [...],
          "recommendations": [...]
        }
      ],
      "consensus": {
        "consolidated_answer": "...",
        "confidence_score": 0.82,
        "agreement_level": 0.91,
        "key_findings": [...],
        "contradictions": [...],
        "agent_votes": {...},
        "minority_opinions": [...],
        "recommendations": [...]
      }
    },
    "how": { ... },
    "why": { ... },
    "general": { ... }
  }
}
```

## ğŸ”§ Configuration

Key settings in `.env`:

```env
# Agent Configuration
MAX_AGENTS=15                    # Maximum number of agents
AGENT_TIMEOUT=300                # Timeout per agent (seconds)
CONSENSUS_THRESHOLD=0.7          # Agreement threshold
MIN_CONFIDENCE_SCORE=0.6         # Minimum confidence to include

# Paper Discovery
MAX_PAPERS_PER_QUERY=50          # Maximum papers to fetch
PAPER_INGESTION_INTERVAL=3600    # Auto-update interval

# Performance
MAX_API_CALLS_PER_MINUTE=60      # Rate limiting
ENABLE_METRICS=true              # Prometheus metrics
```

## ğŸ§ª Example Queries

### Material Science
```json
{
  "query": "transparent conducting oxides",
  "research_question": "What processing parameters optimize the trade-off between electrical conductivity and optical transparency?"
}
```

### AI/ML
```json
{
  "query": "transformer attention mechanisms",
  "research_question": "How do different attention mechanisms affect model performance on long-sequence tasks?"
}
```

### Medicine
```json
{
  "query": "mRNA vaccine delivery systems",
  "research_question": "Why are lipid nanoparticles effective for mRNA delivery and what are the limitations?"
}
```

## ğŸ”¬ Fine-Tuning & Customization

### Domain-Specific Knowledge Injection

The platform supports fine-tuning agents with domain knowledge:

```python
# Future enhancement - fine-tuning interface
from research_platform.models import FineTuner

tuner = FineTuner(base_model="llama-3-70b")
tuner.inject_domain_knowledge(
    domain="materials_science",
    concepts=["AFT0", "TCO", "spray pyrolysis"],
    papers=paper_collection
)
tuner.create_lora_adapter(rank=8, alpha=16)
tuner.save("./data/models/materials_science_v1")
```

## ğŸ”„ Continuous Learning

The platform automatically:
1. **Monitors Model Releases**: Checks provider APIs for new models
2. **Registers New Models**: Adds compatible reasoning models to pool
3. **Ingests New Papers**: Periodically searches for recent publications
4. **Updates Knowledge**: Retrains embeddings and fine-tuned layers
5. **Maintains Audit Trail**: Tracks which models analyzed which papers

## ğŸ“ˆ Monitoring

Access Prometheus metrics at `http://localhost:9090`

Metrics include:
- Papers processed
- Agent response times
- Consensus confidence scores
- API request rates
- Cache hit ratios

## ğŸ§© Extending the Platform

### Adding New Paper Sources

```python
from research_platform.extractors.base import PaperExtractor, Paper

class CustomExtractor(PaperExtractor):
    async def search(self, query: str, max_results: int = 10):
        # Implement search logic
        return papers
    
    async def download_pdf(self, paper: Paper, output_path: str):
        # Implement PDF download
        return success
```

### Adding New Agent Models

```python
from research_platform.agents.base import Agent, AgentConfig

# Register in agent_factory.py
config = AgentConfig(
    agent_type=AgentType.CUSTOM_MODEL,
    api_key="xxx",
    model_name="custom-model-v1"
)
```

## ğŸ› Troubleshooting

### Common Issues

**No agents activated**
- Check API keys in `.env`
- Ensure at least one valid key is provided
- Check logs: `docker-compose logs api`

**PDF download failures**
- Some papers are behind paywalls
- Check internet connectivity
- Verify paper URLs are accessible

**Slow analysis**
- Reduce `MAX_AGENTS` for faster response
- Decrease `max_papers` in requests
- Enable Redis caching

**Database errors**
- Ensure PostgreSQL is running: `docker-compose ps`
- Check connection string in `.env`

## ğŸ“„ API Documentation

Full interactive API docs available at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## ğŸ”’ Security

- API keys stored in environment variables
- Database credentials configurable
- CORS configurable for production
- Rate limiting enabled
- Input validation on all endpoints

## ğŸš¢ Deployment

### Cloud Deployment (AWS/GCP/Azure)

```bash
# Build and push container
docker build -t research-platform:latest .
docker tag research-platform:latest your-registry/research-platform:latest
docker push your-registry/research-platform:latest

# Deploy with managed PostgreSQL and Redis
# Update DATABASE_URL and REDIS_URL in environment
```

### Kubernetes

See `deployment/kubernetes/` for manifests (future enhancement)

## ğŸ“ License

MIT License - see LICENSE file

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## ğŸ“§ Support

For issues and questions:
- GitHub Issues: [Create an issue]
- Documentation: This README
- API Docs: `/docs` endpoint

## ğŸ“ Citation

If you use this platform in research, please cite:

```bibtex
@software{multi_agent_research_platform,
  title={Multi-Agent Research Analysis Platform},
  author={Research Platform Team},
  year={2025},
  version={1.0.0}
}
```

## ğŸ”® Roadmap

- [ ] Web UI for interactive exploration
- [ ] Fine-tuning pipeline for domain adaptation
- [ ] Model performance benchmarking
- [ ] Citation network analysis
- [ ] Automated hypothesis generation
- [ ] Integration with laboratory notebooks
- [ ] Support for more paper sources (IEEE, Springer, Elsevier)
- [ ] Real-time paper monitoring and alerts
- [ ] Collaborative filtering recommendations

---

**Version**: 1.0.0  
**Last Updated**: 2025-01-17  
**Status**: Production Ready âœ…
